{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_validate , cross_val_predict\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import seaborn\n",
    "import os\n",
    "import csv\n",
    "#import re\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "seaborn.set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def get_data(name = ''):\n",
    "    df = \"\"\n",
    "    path = os.getcwd()\n",
    "    direct='data'\n",
    "    file=os.path.join(path,direct, name)\n",
    "    #opencsv=\"\"\n",
    "    try:\n",
    "        opencsv = open(os.path.join(path,direct,name), 'r')\n",
    "        print(opencsv)#find the race-result-horse file\n",
    "    except:\n",
    "        while opencsv != name:  # if the file cant be found if there is an error\n",
    "            print(\"Could not open \", \"file\")\n",
    "            opencsv = input(\"\\nPlease try to open file again: \")\n",
    "    else:\n",
    "        with open(os.path.join(path,direct,name)) as f:\n",
    "            feature_names = []#f.readline()\n",
    "            #print(feature_names)\n",
    "            reader = csv.reader(f)\n",
    "            for row in reader:\n",
    "                feature_names = row\n",
    "                #print(feature_names)\n",
    "                break\n",
    "            df =  pd.read_csv(open(os.path.join(path,direct,name)), header=None, names = feature_names) #, na_values = ['WV-A' , 'WV'])\n",
    "            df.drop(0, inplace = True)\n",
    "    return df\n",
    "    \n",
    "def get_list(df = [] , col_type = ''):\n",
    "    list_set = np.empty(0)\n",
    "    for index, row in df.iterrows():\n",
    "        #print(row[col_type])\n",
    "        if not row[col_type] in list_set:\n",
    "            list_set = np.append(list_set , row[col_type])\n",
    "    return list_set\n",
    "    \n",
    "    \n",
    "    \n",
    "def finish_time(df = pd.DataFrame({})):\n",
    "    for index , row in df.iterrows():\n",
    "        \n",
    "        _min , _sec , _fract = row['finish_time'].split(\".\")\n",
    "       # print(_min , _sec, _fract)\n",
    "        time = (float)( int(_min)*60 + int(_sec) + int(_fract) /100 )\n",
    "        df.loc[index , 'finish_time'] = time\n",
    "        #print(\"df finish_time: \",df.loc[index , 'finish_time'])\n",
    "        #print(type(df.loc[index,'finish_time']) )\n",
    "        \n",
    "\n",
    "def svr_(X, y , X_test , ker = 'linear' , C = 1 , epsilon = 0.1):\n",
    "    svr_model = SVR(kernel = ker , degree=3, gamma = 'auto', coef0=0.0, tol=0.001, C=C,\n",
    "        epsilon= epsilon, shrinking=True, cache_size=200, verbose=False, max_iter=-1)\n",
    "    svr_model.fit(X,y)\n",
    "    print(\"cross_val_score:\")\n",
    "    print(cross_val_score(svr_model , X , y , cv = 10 ,n_jobs = -1 ))\n",
    "    print(\"SVR with {0} kernel score: {1}\".format(ker , svr_model.score(X,y)) )\n",
    "    return svr_model.predict(X_test)\n",
    "    \n",
    "def GBRT_(X,y , X_test, loss = 'ls' , learning_rate = 0.1 , n_estimators = 100 , max_depth = 3):\n",
    "    gbrt_model = GradientBoostingRegressor(loss = loss , learning_rate = learning_rate , \n",
    "                                          n_estimators = n_estimators , max_depth = max_depth)\n",
    "    gbrt_model.fit(X,y)\n",
    "    print(\"gbrt cross_val_score:\")\n",
    "    print(cross_val_score(gbrt_model , X , y , cv = 10 ,n_jobs = -1) )\n",
    "    print(\"GBRT score: {0}\".format(gbrt_model.score(X,y)) )\n",
    "\n",
    "    return gbrt_model.predict(X_test)\n",
    "\n",
    "def evaluation(prediction = [] , label = [] , model = 'SVR' , df = pd.DataFrame({})):\n",
    "    rmse = math.sqrt(mean_squared_error(label  , prediction) )\n",
    "    print(\"\\n\\nMean square error = %.4f\"%rmse)\n",
    "\n",
    "    race_info = pd.DataFrame({\n",
    "        'df_id':df.index.values,\n",
    "        'finish_time':df.loc[:,['finish_time']].values.ravel(),\n",
    "        'prediction':prediction,\n",
    "        'unique_horse_id':df.loc[:,['unique_horse_id']].values.ravel(),\n",
    "        'race_id':df.loc[:,['race_id']].values.ravel(),\n",
    "        'finishing_position':df.loc[:,['finishing_position']].values.ravel()\n",
    "    })\n",
    "    \n",
    "    print(\"Race_info:\")\n",
    "    print(race_info.shape)\n",
    "    \n",
    "    race_list = get_list(race_info , 'race_id')\n",
    "    print(race_list.shape)\n",
    "    \n",
    "    top1_rank = 0\n",
    "    top3_rank = 0\n",
    "    \n",
    "    predict_top1_arr = pd.DataFrame([] , columns = ['unique_horse_id' , 'top1' , 'top3' , 'actual_rank'])\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(race_list.shape[0]):\n",
    "        race_for_sort = race_info[race_info[\"race_id\"] == race_list[i]].reset_index(drop = True)\n",
    "        race_sorted = race_for_sort.sort_values(by = ['prediction'] ).reset_index(drop = True)\n",
    "        #print(race_sorted)\n",
    "        \n",
    "        \n",
    "        \n",
    "        predict_top1_arr.loc[count , ['unique_horse_id']] = race_sorted.loc[0,['unique_horse_id']]\n",
    "        if int(race_sorted.loc[0 , 'finishing_position']) == 1:\n",
    "            predict_top1_arr.loc[count , ['top1']] = True\n",
    "        else:\n",
    "            predict_top1_arr.loc[count , ['top1']] = False\n",
    "        \n",
    "        if int(race_sorted.loc[0 , 'finishing_position']) <=3:\n",
    "            predict_top1_arr.loc[count , ['top3']] = True\n",
    "        else:\n",
    "            predict_top1_arr.loc[count , ['top3']] = False\n",
    "        \n",
    "        predict_top1_arr.loc[count , ['actual_rank']] = int(race_sorted.loc[0 , ['finishing_position']])        \n",
    "        \n",
    "        count = count +1\n",
    "    \n",
    "    #print(predict_top1_arr)\n",
    "    print(\"Top1_accuracy: \" , predict_top1_arr.loc[:,['top1']].values.sum() / predict_top1_arr.shape[0])\n",
    "    print(\"Top3_accuracy: \" , predict_top1_arr.loc[:,['top3']].values.sum()/predict_top1_arr.shape[0])\n",
    "    print(\"Average Rank of all predicted top1 horse: %.4f\" %(predict_top1_arr.loc[:,['actual_rank']].values.sum()/predict_top1_arr.shape[0]) )\n",
    "    \n",
    "    top1_horse_eval = [['horse_id' , 'Top_1' , 'Top_3' , 'ave_rank']]#pd.DataFrame([] , columns = ['unique_horse_id' , 'top1' ,'top3' , 'ave_rank'])\n",
    "    top1_horse_list = get_list(predict_top1_arr , 'unique_horse_id')\n",
    "    count = 0\n",
    "    #print(top1_horse_list.shape)\n",
    "    #print(top1_horse_eval)\n",
    "    \n",
    "    for horse_id in top1_horse_list.flatten():\n",
    "        #print(\"first top1 prediction\",horse_id)\n",
    "        win_horse = predict_top1_arr[predict_top1_arr[\"unique_horse_id\"] == horse_id]\n",
    "        #print(win_horse.shape)\n",
    "        _append = [  horse_id , float(win_horse.loc[:,['top1']].values.sum() / win_horse.shape[0]), \n",
    "                                     float(win_horse.loc[:,['top3']].values.sum() / win_horse.shape[0]),\n",
    "                                    int(round(win_horse.loc[:,['actual_rank']].values.sum() / win_horse.shape[0] ))   ]\n",
    "        #print(np.shape(_append) )\n",
    "        top1_horse_eval.append(_append)\n",
    "        #top1_horse_eval.loc[count , ['unique_horse_id']] = horse_id\n",
    "        #top1_horse_eval.loc[count , ['top1']] = float(win_horse.loc[:,['top1']].values.sum() / win_horse.shape[0])\n",
    "        #top1_horse_eval.loc[count , ['top3']] = float(win_horse.loc[:,['top3']].values.sum() / win_horse.shape[0])\n",
    "        #top1_horse_eval.loc[count , ['ave_rank']] = int(round(win_horse.loc[:,['actual_rank']].values.sum() / win_horse.shape[0] ))\n",
    "        #print(np.shape(top1_horse_eval))\n",
    "        count = count+1\n",
    "        #if count >3:\n",
    "            #break\n",
    "        #print(horse_id)\n",
    "        #print(\"P(Top1) =\", float(win_horse.loc[:,['top1']].values.sum() / win_horse.shape[0]) )\n",
    "        #print(\"P(Top3) =\", float(win_horse.loc[:,['top3']].values.sum() / win_horse.shape[0]) )\n",
    "        #print(\"Average Rank =\", int(round(win_horse.loc[:,['actual_rank']].values.sum() / win_horse.shape[0] )) )\n",
    "        \n",
    "    top1_horse_eval = np.array(top1_horse_eval)\n",
    "    top1_horse_eval = pd.DataFrame(data = top1_horse_eval[1:,] , columns = top1_horse_eval[0 , 0:])\n",
    "    #print(top1_horse_eval)\n",
    "    #return top1_horse_eval\n",
    "    \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "def regression():\n",
    "    df_train = get_data('training.csv') #load data\n",
    "    df_test = get_data('testing.csv')\n",
    "    #print(df_train.describe())\n",
    "    print(\"training set: \",df_train.shape[0])\n",
    "    print(\"testing set: \",df_test.shape[0])\n",
    "    print(get_list(df_test , 'race_id').shape)\n",
    "    finish_time(df_train)  #change finish_time column from str to float\n",
    "    finish_time(df_test)\n",
    "    \n",
    "    X_train_src = df_train.loc[:, ['actual_weight' , 'declared_horse_weight' , 'draw' , 'win_odds' , 'jockey_ave_rank'\n",
    "                                  ,'trainer_ave_rank' , 'recent_ave_rank' , 'race_distance'] ]\n",
    "    y_train_src = df_train.loc[:, ['finish_time'] ]\n",
    "    \n",
    "    X_test_src = df_test.loc[:, ['actual_weight' , 'declared_horse_weight' , 'draw' , 'win_odds' , 'jockey_ave_rank'\n",
    "                                  ,'trainer_ave_rank' , 'recent_ave_rank' , 'race_distance'] ]\n",
    "    \n",
    "    y_test_src = df_test.loc[:, ['finish_time'] ]\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(X_train_src.loc[1].values)\n",
    "    \n",
    "    X_scaler = StandardScaler()\n",
    "    \n",
    "    X_scaler.fit(X_train_src)\n",
    "    \n",
    "    X_train_scaled = X_scaler.transform(X_train_src)  # cannot use test data for StandardScaler fitting so use training data instead\n",
    "    \n",
    "    X_test_scaled = X_scaler.transform(X_test_src)\n",
    "    \n",
    "    X_scaler.fit(y_train_src)\n",
    "    \n",
    "    y_train_scaled = X_scaler.transform(y_train_src)\n",
    "    \n",
    "    y_test_scaled = X_scaler.transform(y_test_src)\n",
    "    \n",
    "    print(X_train_scaled[0] , X_test_scaled[0])\n",
    "    \n",
    "    #svr_(X_train_scaled , y_train_scaled.ravel() , 'linear' , 4 , 0.4 )\n",
    "    svr_wif_norm_predict = svr_(X_train_scaled , y_train_scaled.ravel() ,X_test_scaled   \n",
    "                                                     ,'linear' , 2 , 0.2)   #SVR model   linear: C = 2 , 0.2 best\n",
    "    #X_scaler.fit(y_train_src)\n",
    "    svr_wif_norm_predict = X_scaler.inverse_transform(svr_wif_norm_predict)\n",
    "    svr_wifo_norm_predict =svr_(X_train_src.values , y_train_src.values.ravel() , X_test_src.values ,'rbf', 2, 0.2)\n",
    "    #svr_(X_train_scaled , y_train_scaled.ravel() , 'poly' , 2.5 ,0.1)\n",
    "    #svr_(X_train_scaled , y_train_scaled.ravel() , 'sigmoid' , 2.5 , 0.1)\n",
    "    #svr_(X_train_scaled , y_train_scaled.ravel() , 'precomputed')\n",
    "    \n",
    "    gbrt_wif_norm_predict  = GBRT_(X_train_scaled , y_train_scaled.ravel() ,X_test_scaled,\n",
    "          'lad' , 0.1 , 100 , 3 )  #GBRT model\n",
    "    gbrt_wif_norm_predict = X_scaler.inverse_transform(gbrt_wif_norm_predict)\n",
    "    \n",
    "    gbrt_wifo_norm_predict = GBRT_(X_train_src.values , y_train_src.values.ravel() , X_test_src.values ,\n",
    "          'huber' , 0.25 , 200 , 6 )\n",
    "                     \n",
    "    evaluation(svr_wif_norm_predict , y_test_src.values.ravel() , \"SVR\" ,df_test)  #evaluation part\n",
    "    evaluation(svr_wifo_norm_predict , y_test_src.values.ravel() , \"SVR\" , df_test)\n",
    "    evaluation(gbrt_wif_norm_predict , y_test_src.values.ravel() , 'GBRT' , df_test)\n",
    "    evaluation(grbt_wifo_norm_predict , y_test_src.values.ravel() , 'GBRT' , df_test)\n",
    "    ################################## regression model implementation\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_io.TextIOWrapper name='C:\\\\Users\\\\Tang Ying Kin\\\\Desktop\\\\Item\\\\CSCI3320_PROJECT\\\\csci3320_prj\\\\data\\\\training.csv' mode='r' encoding='cp1252'>\n",
      "<_io.TextIOWrapper name='C:\\\\Users\\\\Tang Ying Kin\\\\Desktop\\\\Item\\\\CSCI3320_PROJECT\\\\csci3320_prj\\\\data\\\\testing.csv' mode='r' encoding='cp1252'>\n",
      "training set:  23500\n",
      "testing set:  5864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tang Ying Kin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480,)\n",
      "['133' '1032' '1' '3.8' '6' '7' '7' '1400']\n",
      "[ 1.59799656 -1.27013737 -1.56862722 -0.83968946 -0.60259412  0.10718446\n",
      "  0.23985213 -0.05447923] [ 0.48147048  0.24724152  0.56572938 -0.7703098  -0.60259412  0.10718446\n",
      " -1.33657774 -0.77413176]\n",
      "cross_val_score:\n",
      "[ 0.9886761   0.9922198   0.99321142  0.99230553  0.9907907   0.99201519\n",
      "  0.9929521   0.99245524  0.98774056  0.99086352]\n",
      "SVR with linear kernel score: 0.9916590853391136\n",
      "cross_val_score:\n",
      "[ 0.06542056  0.04157499 -0.00814377  0.01921192  0.06679134 -0.00596016\n",
      "  0.01848618  0.03765353  0.06092367  0.00637081]\n",
      "SVR with rbf kernel score: 0.2012508955086061\n",
      "gbrt cross_val_score:\n",
      "[ 0.99605389  0.99704577  0.99692725  0.99494801  0.99482901  0.99452528\n",
      "  0.99579665  0.99606928  0.99481171  0.9944164 ]\n",
      "GBRT score: 0.9957398817039307\n",
      "gbrt cross_val_score:\n",
      "[ 0.99579256  0.99697278  0.99666089  0.99508663  0.99455085  0.9945511\n",
      "  0.99565912  0.99588803  0.99438264  0.99426146]\n",
      "GBRT score: 0.9973289991522671\n",
      "\n",
      "\n",
      "Mean square error = 1.8965\n",
      "Race_info:\n",
      "(5864, 6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Tang Ying Kin\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:33: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(480,)\n",
      "Top1_accuracy:  0.20625\n",
      "Top3_accuracy:  0.4666666666666667\n",
      "Average Rank of all predicted top1 horse: 4.6542\n",
      "\n",
      "\n",
      "Mean square error = 18.6836\n",
      "Race_info:\n",
      "(5864, 6)\n",
      "(480,)\n",
      "Top1_accuracy:  0.08333333333333333\n",
      "Top3_accuracy:  0.28541666666666665\n",
      "Average Rank of all predicted top1 horse: 6.3917\n",
      "\n",
      "\n",
      "Mean square error = 1.5624\n",
      "Race_info:\n",
      "(5864, 6)\n",
      "(480,)\n",
      "Top1_accuracy:  0.23541666666666666\n",
      "Top3_accuracy:  0.48541666666666666\n",
      "Average Rank of all predicted top1 horse: 4.5583\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'grbt_wifo_norm_predict' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-9f2e2cda3818>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m     \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchained_assignment\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mregression\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-3f07837cf29f>\u001b[0m in \u001b[0;36mregression\u001b[1;34m()\u001b[0m\n\u001b[0;32m    215\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msvr_wifo_norm_predict\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m\"SVR\"\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    216\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgbrt_wif_norm_predict\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'GBRT'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 217\u001b[1;33m     \u001b[0mevaluation\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgrbt_wifo_norm_predict\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0my_test_src\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m,\u001b[0m \u001b[1;34m'GBRT'\u001b[0m \u001b[1;33m,\u001b[0m \u001b[0mdf_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    218\u001b[0m     \u001b[1;31m################################## regression model implementation\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    219\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'grbt_wifo_norm_predict' is not defined"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    pd.options.mode.chained_assignment = None\n",
    "    regression()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
